{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "SMANNuseSoftmax = True\n",
        "useBatchNorm = True   #default: True\n",
        "if(SMANNuseSoftmax):\n",
        "\tuseBatchNorm = True #False\n",
        "\tusePositiveWeights = True\n",
        "\tif(usePositiveWeights):\n",
        "\t\tusePositiveWeightsClampModel = True\n",
        "\n",
        "debugUsePositiveWeightsVerify = False\n",
        "\n",
        "def weightsSetPositiveModel(self):\n",
        "\tif(usePositiveWeights):\n",
        "\t\tif(usePositiveWeightsClampModel):\n",
        "\t\t\tfor p in self.parameters():\n",
        "\t\t\t\tp.data.clamp_(0)"
      ],
      "metadata": {
        "id": "xiLUWEgzH7su"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VqrtWo-D_8Cj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "297a439e-b5b8-4564-8c97-272b44342a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/80], Step [100/500] Loss: 2.2735\n",
            "Epoch [1/80], Step [200/500] Loss: 2.2202\n",
            "Epoch [1/80], Step [300/500] Loss: 2.1626\n",
            "Epoch [1/80], Step [400/500] Loss: 2.1516\n",
            "Epoch [1/80], Step [500/500] Loss: 2.1172\n",
            "Epoch [2/80], Step [100/500] Loss: 2.0545\n",
            "Epoch [2/80], Step [200/500] Loss: 1.9979\n",
            "Epoch [2/80], Step [300/500] Loss: 1.9785\n",
            "Epoch [2/80], Step [400/500] Loss: 1.9411\n",
            "Epoch [2/80], Step [500/500] Loss: 2.0090\n",
            "Epoch [3/80], Step [100/500] Loss: 1.8548\n",
            "Epoch [3/80], Step [200/500] Loss: 1.7817\n",
            "Epoch [3/80], Step [300/500] Loss: 1.7678\n",
            "Epoch [3/80], Step [400/500] Loss: 1.7073\n",
            "Epoch [3/80], Step [500/500] Loss: 1.7743\n",
            "Epoch [4/80], Step [100/500] Loss: 1.6654\n",
            "Epoch [4/80], Step [200/500] Loss: 1.5601\n",
            "Epoch [4/80], Step [300/500] Loss: 1.6052\n",
            "Epoch [4/80], Step [400/500] Loss: 1.6121\n",
            "Epoch [4/80], Step [500/500] Loss: 1.4307\n",
            "Epoch [5/80], Step [100/500] Loss: 1.6468\n",
            "Epoch [5/80], Step [200/500] Loss: 1.4416\n",
            "Epoch [5/80], Step [300/500] Loss: 1.3146\n",
            "Epoch [5/80], Step [400/500] Loss: 1.3799\n",
            "Epoch [5/80], Step [500/500] Loss: 1.5314\n",
            "Epoch [6/80], Step [100/500] Loss: 1.4350\n",
            "Epoch [6/80], Step [200/500] Loss: 1.2166\n",
            "Epoch [6/80], Step [300/500] Loss: 1.1467\n",
            "Epoch [6/80], Step [400/500] Loss: 1.0955\n",
            "Epoch [6/80], Step [500/500] Loss: 1.1442\n",
            "Epoch [7/80], Step [100/500] Loss: 1.1139\n",
            "Epoch [7/80], Step [200/500] Loss: 1.2595\n",
            "Epoch [7/80], Step [300/500] Loss: 1.1266\n",
            "Epoch [7/80], Step [400/500] Loss: 1.1730\n",
            "Epoch [7/80], Step [500/500] Loss: 1.2180\n",
            "Epoch [8/80], Step [100/500] Loss: 1.1322\n",
            "Epoch [8/80], Step [200/500] Loss: 1.0345\n",
            "Epoch [8/80], Step [300/500] Loss: 1.0004\n",
            "Epoch [8/80], Step [400/500] Loss: 1.2438\n",
            "Epoch [8/80], Step [500/500] Loss: 0.9632\n",
            "Epoch [9/80], Step [100/500] Loss: 0.8217\n",
            "Epoch [9/80], Step [200/500] Loss: 0.8550\n",
            "Epoch [9/80], Step [300/500] Loss: 0.9958\n",
            "Epoch [9/80], Step [400/500] Loss: 1.1335\n",
            "Epoch [9/80], Step [500/500] Loss: 1.0634\n",
            "Epoch [10/80], Step [100/500] Loss: 0.8192\n",
            "Epoch [10/80], Step [200/500] Loss: 0.9197\n",
            "Epoch [10/80], Step [300/500] Loss: 1.0992\n",
            "Epoch [10/80], Step [400/500] Loss: 0.8395\n",
            "Epoch [10/80], Step [500/500] Loss: 0.9540\n",
            "Epoch [11/80], Step [100/500] Loss: 0.9404\n",
            "Epoch [11/80], Step [200/500] Loss: 0.8069\n",
            "Epoch [11/80], Step [300/500] Loss: 0.6847\n",
            "Epoch [11/80], Step [400/500] Loss: 0.7907\n",
            "Epoch [11/80], Step [500/500] Loss: 0.9379\n",
            "Epoch [12/80], Step [100/500] Loss: 0.7274\n",
            "Epoch [12/80], Step [200/500] Loss: 0.8590\n",
            "Epoch [12/80], Step [300/500] Loss: 0.7946\n",
            "Epoch [12/80], Step [400/500] Loss: 0.8782\n",
            "Epoch [12/80], Step [500/500] Loss: 0.8184\n",
            "Epoch [13/80], Step [100/500] Loss: 0.7120\n",
            "Epoch [13/80], Step [200/500] Loss: 0.7526\n",
            "Epoch [13/80], Step [300/500] Loss: 0.8658\n",
            "Epoch [13/80], Step [400/500] Loss: 0.7757\n",
            "Epoch [13/80], Step [500/500] Loss: 0.9064\n",
            "Epoch [14/80], Step [100/500] Loss: 0.6857\n",
            "Epoch [14/80], Step [200/500] Loss: 0.8385\n",
            "Epoch [14/80], Step [300/500] Loss: 0.7030\n",
            "Epoch [14/80], Step [400/500] Loss: 0.6542\n",
            "Epoch [14/80], Step [500/500] Loss: 0.8494\n",
            "Epoch [15/80], Step [100/500] Loss: 0.7404\n",
            "Epoch [15/80], Step [200/500] Loss: 0.6532\n",
            "Epoch [15/80], Step [300/500] Loss: 0.7254\n",
            "Epoch [15/80], Step [400/500] Loss: 0.5892\n",
            "Epoch [15/80], Step [500/500] Loss: 0.8323\n",
            "Epoch [16/80], Step [100/500] Loss: 0.6708\n",
            "Epoch [16/80], Step [200/500] Loss: 0.7067\n",
            "Epoch [16/80], Step [300/500] Loss: 0.6690\n",
            "Epoch [16/80], Step [400/500] Loss: 0.7073\n",
            "Epoch [16/80], Step [500/500] Loss: 0.6477\n",
            "Epoch [17/80], Step [100/500] Loss: 0.6239\n",
            "Epoch [17/80], Step [200/500] Loss: 0.7276\n",
            "Epoch [17/80], Step [300/500] Loss: 0.7813\n",
            "Epoch [17/80], Step [400/500] Loss: 0.7195\n",
            "Epoch [17/80], Step [500/500] Loss: 0.6786\n",
            "Epoch [18/80], Step [100/500] Loss: 0.5757\n",
            "Epoch [18/80], Step [200/500] Loss: 0.5509\n",
            "Epoch [18/80], Step [300/500] Loss: 0.5655\n",
            "Epoch [18/80], Step [400/500] Loss: 0.6470\n",
            "Epoch [18/80], Step [500/500] Loss: 0.5244\n",
            "Epoch [19/80], Step [100/500] Loss: 0.5183\n",
            "Epoch [19/80], Step [200/500] Loss: 0.5643\n",
            "Epoch [19/80], Step [300/500] Loss: 0.7093\n",
            "Epoch [19/80], Step [400/500] Loss: 0.4554\n",
            "Epoch [19/80], Step [500/500] Loss: 0.5508\n",
            "Epoch [20/80], Step [100/500] Loss: 0.7509\n",
            "Epoch [20/80], Step [200/500] Loss: 0.5671\n",
            "Epoch [20/80], Step [300/500] Loss: 0.6845\n",
            "Epoch [20/80], Step [400/500] Loss: 0.5753\n",
            "Epoch [20/80], Step [500/500] Loss: 0.5038\n",
            "Epoch [21/80], Step [100/500] Loss: 0.6386\n",
            "Epoch [21/80], Step [200/500] Loss: 0.7349\n",
            "Epoch [21/80], Step [300/500] Loss: 0.5938\n",
            "Epoch [21/80], Step [400/500] Loss: 0.7286\n",
            "Epoch [21/80], Step [500/500] Loss: 0.5521\n",
            "Epoch [22/80], Step [100/500] Loss: 0.6892\n",
            "Epoch [22/80], Step [200/500] Loss: 0.5174\n",
            "Epoch [22/80], Step [300/500] Loss: 0.4484\n",
            "Epoch [22/80], Step [400/500] Loss: 0.5035\n",
            "Epoch [22/80], Step [500/500] Loss: 0.5430\n",
            "Epoch [23/80], Step [100/500] Loss: 0.6552\n",
            "Epoch [23/80], Step [200/500] Loss: 0.5634\n",
            "Epoch [23/80], Step [300/500] Loss: 0.5705\n",
            "Epoch [23/80], Step [400/500] Loss: 0.4597\n",
            "Epoch [23/80], Step [500/500] Loss: 0.4667\n",
            "Epoch [24/80], Step [100/500] Loss: 0.5254\n",
            "Epoch [24/80], Step [200/500] Loss: 0.6558\n",
            "Epoch [24/80], Step [300/500] Loss: 0.5220\n",
            "Epoch [24/80], Step [400/500] Loss: 0.4171\n",
            "Epoch [24/80], Step [500/500] Loss: 0.4900\n",
            "Epoch [25/80], Step [100/500] Loss: 0.5350\n",
            "Epoch [25/80], Step [200/500] Loss: 0.5821\n",
            "Epoch [25/80], Step [300/500] Loss: 0.5348\n",
            "Epoch [25/80], Step [400/500] Loss: 0.5326\n",
            "Epoch [25/80], Step [500/500] Loss: 0.5574\n",
            "Epoch [26/80], Step [100/500] Loss: 0.5332\n",
            "Epoch [26/80], Step [200/500] Loss: 0.5317\n",
            "Epoch [26/80], Step [300/500] Loss: 0.3855\n",
            "Epoch [26/80], Step [400/500] Loss: 0.4765\n",
            "Epoch [26/80], Step [500/500] Loss: 0.5780\n",
            "Epoch [27/80], Step [100/500] Loss: 0.4281\n",
            "Epoch [27/80], Step [200/500] Loss: 0.5258\n",
            "Epoch [27/80], Step [300/500] Loss: 0.4556\n",
            "Epoch [27/80], Step [400/500] Loss: 0.5914\n",
            "Epoch [27/80], Step [500/500] Loss: 0.3944\n",
            "Epoch [28/80], Step [100/500] Loss: 0.4209\n",
            "Epoch [28/80], Step [200/500] Loss: 0.4645\n",
            "Epoch [28/80], Step [300/500] Loss: 0.4017\n",
            "Epoch [28/80], Step [400/500] Loss: 0.4612\n",
            "Epoch [28/80], Step [500/500] Loss: 0.4250\n",
            "Epoch [29/80], Step [100/500] Loss: 0.4334\n",
            "Epoch [29/80], Step [200/500] Loss: 0.4728\n",
            "Epoch [29/80], Step [300/500] Loss: 0.4061\n",
            "Epoch [29/80], Step [400/500] Loss: 0.5724\n",
            "Epoch [29/80], Step [500/500] Loss: 0.4277\n",
            "Epoch [30/80], Step [100/500] Loss: 0.4953\n",
            "Epoch [30/80], Step [200/500] Loss: 0.3861\n",
            "Epoch [30/80], Step [300/500] Loss: 0.3828\n",
            "Epoch [30/80], Step [400/500] Loss: 0.5098\n",
            "Epoch [30/80], Step [500/500] Loss: 0.3445\n",
            "Epoch [31/80], Step [100/500] Loss: 0.5115\n",
            "Epoch [31/80], Step [200/500] Loss: 0.3933\n",
            "Epoch [31/80], Step [300/500] Loss: 0.6072\n",
            "Epoch [31/80], Step [400/500] Loss: 0.5247\n",
            "Epoch [31/80], Step [500/500] Loss: 0.3628\n",
            "Epoch [32/80], Step [100/500] Loss: 0.3743\n",
            "Epoch [32/80], Step [200/500] Loss: 0.5538\n",
            "Epoch [32/80], Step [300/500] Loss: 0.4861\n",
            "Epoch [32/80], Step [400/500] Loss: 0.5169\n",
            "Epoch [32/80], Step [500/500] Loss: 0.5711\n",
            "Epoch [33/80], Step [100/500] Loss: 0.3989\n",
            "Epoch [33/80], Step [200/500] Loss: 0.4096\n",
            "Epoch [33/80], Step [300/500] Loss: 0.5553\n",
            "Epoch [33/80], Step [400/500] Loss: 0.4191\n",
            "Epoch [33/80], Step [500/500] Loss: 0.4054\n",
            "Epoch [34/80], Step [100/500] Loss: 0.4351\n",
            "Epoch [34/80], Step [200/500] Loss: 0.5476\n",
            "Epoch [34/80], Step [300/500] Loss: 0.4513\n",
            "Epoch [34/80], Step [400/500] Loss: 0.4689\n",
            "Epoch [34/80], Step [500/500] Loss: 0.4950\n",
            "Epoch [35/80], Step [100/500] Loss: 0.3489\n",
            "Epoch [35/80], Step [200/500] Loss: 0.4426\n",
            "Epoch [35/80], Step [300/500] Loss: 0.5339\n",
            "Epoch [35/80], Step [400/500] Loss: 0.3485\n",
            "Epoch [35/80], Step [500/500] Loss: 0.3612\n",
            "Epoch [36/80], Step [100/500] Loss: 0.4060\n",
            "Epoch [36/80], Step [200/500] Loss: 0.4385\n",
            "Epoch [36/80], Step [300/500] Loss: 0.3180\n",
            "Epoch [36/80], Step [400/500] Loss: 0.3740\n",
            "Epoch [36/80], Step [500/500] Loss: 0.4889\n",
            "Epoch [37/80], Step [100/500] Loss: 0.4828\n",
            "Epoch [37/80], Step [200/500] Loss: 0.3947\n",
            "Epoch [37/80], Step [300/500] Loss: 0.4439\n",
            "Epoch [37/80], Step [400/500] Loss: 0.5262\n",
            "Epoch [37/80], Step [500/500] Loss: 0.4816\n",
            "Epoch [38/80], Step [100/500] Loss: 0.4140\n",
            "Epoch [38/80], Step [200/500] Loss: 0.4031\n",
            "Epoch [38/80], Step [300/500] Loss: 0.4459\n",
            "Epoch [38/80], Step [400/500] Loss: 0.4650\n",
            "Epoch [38/80], Step [500/500] Loss: 0.6426\n",
            "Epoch [39/80], Step [100/500] Loss: 0.5480\n",
            "Epoch [39/80], Step [200/500] Loss: 0.4660\n",
            "Epoch [39/80], Step [300/500] Loss: 0.4122\n",
            "Epoch [39/80], Step [400/500] Loss: 0.5730\n",
            "Epoch [39/80], Step [500/500] Loss: 0.4303\n",
            "Epoch [40/80], Step [100/500] Loss: 0.3479\n",
            "Epoch [40/80], Step [200/500] Loss: 0.4314\n",
            "Epoch [40/80], Step [300/500] Loss: 0.2900\n",
            "Epoch [40/80], Step [400/500] Loss: 0.4255\n",
            "Epoch [40/80], Step [500/500] Loss: 0.5589\n",
            "Epoch [41/80], Step [100/500] Loss: 0.3409\n",
            "Epoch [41/80], Step [200/500] Loss: 0.3576\n",
            "Epoch [41/80], Step [300/500] Loss: 0.3827\n",
            "Epoch [41/80], Step [400/500] Loss: 0.2505\n",
            "Epoch [41/80], Step [500/500] Loss: 0.4635\n",
            "Epoch [42/80], Step [100/500] Loss: 0.4070\n",
            "Epoch [42/80], Step [200/500] Loss: 0.3734\n",
            "Epoch [42/80], Step [300/500] Loss: 0.3117\n",
            "Epoch [42/80], Step [400/500] Loss: 0.2749\n",
            "Epoch [42/80], Step [500/500] Loss: 0.4549\n",
            "Epoch [43/80], Step [100/500] Loss: 0.2319\n",
            "Epoch [43/80], Step [200/500] Loss: 0.3262\n",
            "Epoch [43/80], Step [300/500] Loss: 0.2569\n",
            "Epoch [43/80], Step [400/500] Loss: 0.3056\n",
            "Epoch [43/80], Step [500/500] Loss: 0.4721\n",
            "Epoch [44/80], Step [100/500] Loss: 0.3879\n",
            "Epoch [44/80], Step [200/500] Loss: 0.3190\n",
            "Epoch [44/80], Step [300/500] Loss: 0.2898\n",
            "Epoch [44/80], Step [400/500] Loss: 0.3938\n",
            "Epoch [44/80], Step [500/500] Loss: 0.3552\n",
            "Epoch [45/80], Step [100/500] Loss: 0.4810\n",
            "Epoch [45/80], Step [200/500] Loss: 0.3440\n",
            "Epoch [45/80], Step [300/500] Loss: 0.2855\n",
            "Epoch [45/80], Step [400/500] Loss: 0.2574\n",
            "Epoch [45/80], Step [500/500] Loss: 0.3283\n",
            "Epoch [46/80], Step [100/500] Loss: 0.5209\n",
            "Epoch [46/80], Step [200/500] Loss: 0.4179\n",
            "Epoch [46/80], Step [300/500] Loss: 0.4423\n",
            "Epoch [46/80], Step [400/500] Loss: 0.4638\n",
            "Epoch [46/80], Step [500/500] Loss: 0.4064\n",
            "Epoch [47/80], Step [100/500] Loss: 0.3175\n",
            "Epoch [47/80], Step [200/500] Loss: 0.2877\n",
            "Epoch [47/80], Step [300/500] Loss: 0.2566\n",
            "Epoch [47/80], Step [400/500] Loss: 0.4118\n",
            "Epoch [47/80], Step [500/500] Loss: 0.3793\n",
            "Epoch [48/80], Step [100/500] Loss: 0.3257\n",
            "Epoch [48/80], Step [200/500] Loss: 0.3581\n",
            "Epoch [48/80], Step [300/500] Loss: 0.4021\n",
            "Epoch [48/80], Step [400/500] Loss: 0.3856\n",
            "Epoch [48/80], Step [500/500] Loss: 0.3777\n",
            "Epoch [49/80], Step [100/500] Loss: 0.3955\n",
            "Epoch [49/80], Step [200/500] Loss: 0.2432\n",
            "Epoch [49/80], Step [300/500] Loss: 0.3137\n",
            "Epoch [49/80], Step [400/500] Loss: 0.3838\n",
            "Epoch [49/80], Step [500/500] Loss: 0.4146\n",
            "Epoch [50/80], Step [100/500] Loss: 0.4655\n",
            "Epoch [50/80], Step [200/500] Loss: 0.3920\n",
            "Epoch [50/80], Step [300/500] Loss: 0.3864\n",
            "Epoch [50/80], Step [400/500] Loss: 0.4900\n",
            "Epoch [50/80], Step [500/500] Loss: 0.2957\n",
            "Epoch [51/80], Step [100/500] Loss: 0.3651\n",
            "Epoch [51/80], Step [200/500] Loss: 0.3785\n",
            "Epoch [51/80], Step [300/500] Loss: 0.4454\n",
            "Epoch [51/80], Step [400/500] Loss: 0.4422\n",
            "Epoch [51/80], Step [500/500] Loss: 0.4126\n",
            "Epoch [52/80], Step [100/500] Loss: 0.4997\n",
            "Epoch [52/80], Step [200/500] Loss: 0.3533\n",
            "Epoch [52/80], Step [300/500] Loss: 0.4134\n",
            "Epoch [52/80], Step [400/500] Loss: 0.3213\n",
            "Epoch [52/80], Step [500/500] Loss: 0.3763\n",
            "Epoch [53/80], Step [100/500] Loss: 0.4005\n",
            "Epoch [53/80], Step [200/500] Loss: 0.4442\n",
            "Epoch [53/80], Step [300/500] Loss: 0.3695\n",
            "Epoch [53/80], Step [400/500] Loss: 0.5459\n",
            "Epoch [53/80], Step [500/500] Loss: 0.4881\n",
            "Epoch [54/80], Step [100/500] Loss: 0.2519\n",
            "Epoch [54/80], Step [200/500] Loss: 0.2233\n",
            "Epoch [54/80], Step [300/500] Loss: 0.2955\n",
            "Epoch [54/80], Step [400/500] Loss: 0.3863\n",
            "Epoch [54/80], Step [500/500] Loss: 0.5657\n",
            "Epoch [55/80], Step [100/500] Loss: 0.3118\n",
            "Epoch [55/80], Step [200/500] Loss: 0.4059\n",
            "Epoch [55/80], Step [300/500] Loss: 0.3426\n",
            "Epoch [55/80], Step [400/500] Loss: 0.4254\n",
            "Epoch [55/80], Step [500/500] Loss: 0.3899\n",
            "Epoch [56/80], Step [100/500] Loss: 0.4466\n",
            "Epoch [56/80], Step [200/500] Loss: 0.3669\n",
            "Epoch [56/80], Step [300/500] Loss: 0.4610\n",
            "Epoch [56/80], Step [400/500] Loss: 0.3169\n",
            "Epoch [56/80], Step [500/500] Loss: 0.3904\n",
            "Epoch [57/80], Step [100/500] Loss: 0.3158\n",
            "Epoch [57/80], Step [200/500] Loss: 0.2843\n",
            "Epoch [57/80], Step [300/500] Loss: 0.4845\n",
            "Epoch [57/80], Step [400/500] Loss: 0.4639\n",
            "Epoch [57/80], Step [500/500] Loss: 0.3605\n",
            "Epoch [58/80], Step [100/500] Loss: 0.4479\n",
            "Epoch [58/80], Step [200/500] Loss: 0.4316\n",
            "Epoch [58/80], Step [300/500] Loss: 0.4467\n",
            "Epoch [58/80], Step [400/500] Loss: 0.3922\n",
            "Epoch [58/80], Step [500/500] Loss: 0.3316\n",
            "Epoch [59/80], Step [100/500] Loss: 0.4286\n",
            "Epoch [59/80], Step [200/500] Loss: 0.3985\n",
            "Epoch [59/80], Step [300/500] Loss: 0.4512\n",
            "Epoch [59/80], Step [400/500] Loss: 0.3032\n",
            "Epoch [59/80], Step [500/500] Loss: 0.3026\n",
            "Epoch [60/80], Step [100/500] Loss: 0.3590\n",
            "Epoch [60/80], Step [200/500] Loss: 0.3913\n",
            "Epoch [60/80], Step [300/500] Loss: 0.3339\n",
            "Epoch [60/80], Step [400/500] Loss: 0.3708\n",
            "Epoch [60/80], Step [500/500] Loss: 0.3300\n",
            "Epoch [61/80], Step [100/500] Loss: 0.3701\n",
            "Epoch [61/80], Step [200/500] Loss: 0.2792\n",
            "Epoch [61/80], Step [300/500] Loss: 0.3735\n",
            "Epoch [61/80], Step [400/500] Loss: 0.3284\n",
            "Epoch [61/80], Step [500/500] Loss: 0.2417\n",
            "Epoch [62/80], Step [100/500] Loss: 0.3124\n",
            "Epoch [62/80], Step [200/500] Loss: 0.3675\n",
            "Epoch [62/80], Step [300/500] Loss: 0.3236\n",
            "Epoch [62/80], Step [400/500] Loss: 0.4841\n",
            "Epoch [62/80], Step [500/500] Loss: 0.5398\n",
            "Epoch [63/80], Step [100/500] Loss: 0.3914\n",
            "Epoch [63/80], Step [200/500] Loss: 0.3178\n",
            "Epoch [63/80], Step [300/500] Loss: 0.3128\n",
            "Epoch [63/80], Step [400/500] Loss: 0.3800\n",
            "Epoch [63/80], Step [500/500] Loss: 0.3083\n",
            "Epoch [64/80], Step [100/500] Loss: 0.2510\n",
            "Epoch [64/80], Step [200/500] Loss: 0.4584\n",
            "Epoch [64/80], Step [300/500] Loss: 0.2699\n",
            "Epoch [64/80], Step [400/500] Loss: 0.3547\n",
            "Epoch [64/80], Step [500/500] Loss: 0.3062\n",
            "Epoch [65/80], Step [100/500] Loss: 0.2983\n",
            "Epoch [65/80], Step [200/500] Loss: 0.4508\n",
            "Epoch [65/80], Step [300/500] Loss: 0.3432\n",
            "Epoch [65/80], Step [400/500] Loss: 0.4227\n",
            "Epoch [65/80], Step [500/500] Loss: 0.2024\n",
            "Epoch [66/80], Step [100/500] Loss: 0.3281\n",
            "Epoch [66/80], Step [200/500] Loss: 0.2374\n",
            "Epoch [66/80], Step [300/500] Loss: 0.2286\n",
            "Epoch [66/80], Step [400/500] Loss: 0.2300\n",
            "Epoch [66/80], Step [500/500] Loss: 0.3308\n",
            "Epoch [67/80], Step [100/500] Loss: 0.2334\n",
            "Epoch [67/80], Step [200/500] Loss: 0.3839\n",
            "Epoch [67/80], Step [300/500] Loss: 0.3719\n",
            "Epoch [67/80], Step [400/500] Loss: 0.3496\n",
            "Epoch [67/80], Step [500/500] Loss: 0.3325\n",
            "Epoch [68/80], Step [100/500] Loss: 0.2964\n",
            "Epoch [68/80], Step [200/500] Loss: 0.3146\n",
            "Epoch [68/80], Step [300/500] Loss: 0.2966\n",
            "Epoch [68/80], Step [400/500] Loss: 0.3449\n",
            "Epoch [68/80], Step [500/500] Loss: 0.2111\n",
            "Epoch [69/80], Step [100/500] Loss: 0.2120\n",
            "Epoch [69/80], Step [200/500] Loss: 0.2514\n",
            "Epoch [69/80], Step [300/500] Loss: 0.3597\n",
            "Epoch [69/80], Step [400/500] Loss: 0.2664\n",
            "Epoch [69/80], Step [500/500] Loss: 0.4348\n",
            "Epoch [70/80], Step [100/500] Loss: 0.3103\n",
            "Epoch [70/80], Step [200/500] Loss: 0.3410\n",
            "Epoch [70/80], Step [300/500] Loss: 0.3041\n",
            "Epoch [70/80], Step [400/500] Loss: 0.3291\n",
            "Epoch [70/80], Step [500/500] Loss: 0.3851\n",
            "Epoch [71/80], Step [100/500] Loss: 0.2656\n",
            "Epoch [71/80], Step [200/500] Loss: 0.4331\n",
            "Epoch [71/80], Step [300/500] Loss: 0.4154\n",
            "Epoch [71/80], Step [400/500] Loss: 0.4375\n",
            "Epoch [71/80], Step [500/500] Loss: 0.3278\n",
            "Epoch [72/80], Step [100/500] Loss: 0.2817\n",
            "Epoch [72/80], Step [200/500] Loss: 0.3034\n",
            "Epoch [72/80], Step [300/500] Loss: 0.2953\n",
            "Epoch [72/80], Step [400/500] Loss: 0.3168\n",
            "Epoch [72/80], Step [500/500] Loss: 0.3724\n",
            "Epoch [73/80], Step [100/500] Loss: 0.2263\n",
            "Epoch [73/80], Step [200/500] Loss: 0.2794\n",
            "Epoch [73/80], Step [300/500] Loss: 0.2854\n",
            "Epoch [73/80], Step [400/500] Loss: 0.3236\n",
            "Epoch [73/80], Step [500/500] Loss: 0.3528\n",
            "Epoch [74/80], Step [100/500] Loss: 0.2783\n",
            "Epoch [74/80], Step [200/500] Loss: 0.3163\n",
            "Epoch [74/80], Step [300/500] Loss: 0.2696\n",
            "Epoch [74/80], Step [400/500] Loss: 0.3360\n",
            "Epoch [74/80], Step [500/500] Loss: 0.4316\n",
            "Epoch [75/80], Step [100/500] Loss: 0.2755\n",
            "Epoch [75/80], Step [200/500] Loss: 0.3466\n",
            "Epoch [75/80], Step [300/500] Loss: 0.3490\n",
            "Epoch [75/80], Step [400/500] Loss: 0.3862\n",
            "Epoch [75/80], Step [500/500] Loss: 0.5578\n",
            "Epoch [76/80], Step [100/500] Loss: 0.3130\n",
            "Epoch [76/80], Step [200/500] Loss: 0.3282\n",
            "Epoch [76/80], Step [300/500] Loss: 0.4422\n",
            "Epoch [76/80], Step [400/500] Loss: 0.4321\n",
            "Epoch [76/80], Step [500/500] Loss: 0.3297\n",
            "Epoch [77/80], Step [100/500] Loss: 0.3016\n",
            "Epoch [77/80], Step [200/500] Loss: 0.1244\n",
            "Epoch [77/80], Step [300/500] Loss: 0.4242\n",
            "Epoch [77/80], Step [400/500] Loss: 0.2935\n",
            "Epoch [77/80], Step [500/500] Loss: 0.2356\n",
            "Epoch [78/80], Step [100/500] Loss: 0.2882\n",
            "Epoch [78/80], Step [200/500] Loss: 0.1812\n",
            "Epoch [78/80], Step [300/500] Loss: 0.2149\n",
            "Epoch [78/80], Step [400/500] Loss: 0.3184\n",
            "Epoch [78/80], Step [500/500] Loss: 0.4288\n",
            "Epoch [79/80], Step [100/500] Loss: 0.2814\n",
            "Epoch [79/80], Step [200/500] Loss: 0.2769\n",
            "Epoch [79/80], Step [300/500] Loss: 0.4054\n",
            "Epoch [79/80], Step [400/500] Loss: 0.3015\n",
            "Epoch [79/80], Step [500/500] Loss: 0.4031\n",
            "Epoch [80/80], Step [100/500] Loss: 0.3553\n",
            "Epoch [80/80], Step [200/500] Loss: 0.2646\n",
            "Epoch [80/80], Step [300/500] Loss: 0.3544\n",
            "Epoch [80/80], Step [400/500] Loss: 0.2987\n",
            "Epoch [80/80], Step [500/500] Loss: 0.3964\n",
            "Accuracy of the model on the test images: 86.08 %\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------------------- #\n",
        "# An implementation of https://arxiv.org/pdf/1512.03385.pdf\t\t\t\t\t#\n",
        "# See section 4.2 for the model architecture on CIFAR-10\t\t\t\t\t   #\n",
        "# Some part of the code was referenced from below\t\t\t\t\t\t\t  #\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py   #\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 80\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Image preprocessing modules\n",
        "transform = transforms.Compose([\n",
        "\ttransforms.Pad(4),\n",
        "\ttransforms.RandomHorizontalFlip(),\n",
        "\ttransforms.RandomCrop(32),\n",
        "\ttransforms.ToTensor()])\n",
        "\n",
        "# CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "\t\t\t\t\t\t\t\t\t\t\t train=True, \n",
        "\t\t\t\t\t\t\t\t\t\t\t transform=transform,\n",
        "\t\t\t\t\t\t\t\t\t\t\t download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "\t\t\t\t\t\t\t\t\t\t\ttrain=False, \n",
        "\t\t\t\t\t\t\t\t\t\t\ttransform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "\t\t\t\t\t\t\t\t\t\t   batch_size=100, \n",
        "\t\t\t\t\t\t\t\t\t\t   shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "\t\t\t\t\t\t\t\t\t\t  batch_size=100, \n",
        "\t\t\t\t\t\t\t\t\t\t  shuffle=False)\n",
        "\n",
        "# 3x3 convolution\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "\treturn nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
        "\t\t\t\t\t stride=stride, padding=1, bias=False)\n",
        "\n",
        "# Residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "\tdef __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "\t\tsuper(ResidualBlock, self).__init__()\n",
        "\t\tself.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "\t\tif(useBatchNorm):\n",
        "\t\t\tself.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\t\tif(SMANNuseSoftmax):\n",
        "\t\t\tself.relu = nn.Softmax(dim=1)\n",
        "\t\telse:\n",
        "\t\t\tself.relu = nn.ReLU(inplace=True)\n",
        "\t\tself.conv2 = conv3x3(out_channels, out_channels)\n",
        "\t\tif(useBatchNorm):\n",
        "\t\t\tself.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\t\tself.downsample = downsample\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tresidual = x\n",
        "\t\tout = self.conv1(x)\n",
        "\t\tif(useBatchNorm):\n",
        "\t\t\tout = self.bn1(out)\n",
        "\t\tout = self.relu(out)\n",
        "\t\tout = self.conv2(out)\n",
        "\t\tif(debugUsePositiveWeightsVerify):\n",
        "\t\t\tprint(\"self.conv2.weight = \", self.conv2.weight)\n",
        "\t\tif(useBatchNorm):\n",
        "\t\t\tout = self.bn2(out)\n",
        "\t\tif self.downsample:\n",
        "\t\t\tresidual = self.downsample(x)\n",
        "\t\tout += residual\n",
        "\t\tout = self.relu(out)\n",
        "\t\treturn out\n",
        "\n",
        "# ResNet\n",
        "class ResNet(nn.Module):\n",
        "\tdef __init__(self, block, layers, num_classes=10):\n",
        "\t\tsuper(ResNet, self).__init__()\n",
        "\t\tself.in_channels = 16\n",
        "\t\tself.conv = conv3x3(3, 16)\n",
        "\t\tif(useBatchNorm):\n",
        "\t\t\tself.bn = nn.BatchNorm2d(16)\n",
        "\t\tif(SMANNuseSoftmax):\n",
        "\t\t\tself.relu = nn.Softmax(dim=1)\n",
        "\t\telse:\n",
        "\t\t\tself.relu = nn.ReLU(inplace=True)\n",
        "\t\tself.layer1 = self.make_layer(block, 16, layers[0])\n",
        "\t\tself.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
        "\t\tself.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
        "\t\tself.avg_pool = nn.AvgPool2d(8)\n",
        "\t\tself.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "\t\tweightsSetPositiveModel(self)\n",
        "\n",
        "\tdef make_layer(self, block, out_channels, blocks, stride=1):\n",
        "\t\tdownsample = None\n",
        "\t\tif (stride != 1) or (self.in_channels != out_channels):\n",
        "\t\t\tdownsample = nn.Sequential(\n",
        "\t\t\t\tconv3x3(self.in_channels, out_channels, stride=stride),\n",
        "\t\t\t\tnn.BatchNorm2d(out_channels))\n",
        "\t\tlayers = []\n",
        "\t\tlayers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "\t\tself.in_channels = out_channels\n",
        "\t\tfor i in range(1, blocks):\n",
        "\t\t\tlayers.append(block(out_channels, out_channels))\n",
        "\t\treturn nn.Sequential(*layers)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = self.conv(x)\n",
        "\t\tif(useBatchNorm):\n",
        "\t\t\tout = self.bn(out)\n",
        "\t\tout = self.relu(out)\n",
        "\t\tout = self.layer1(out)\n",
        "\t\tout = self.layer2(out)\n",
        "\t\tout = self.layer3(out)\n",
        "\t\tout = self.avg_pool(out)\n",
        "\t\tout = out.view(out.size(0), -1)\n",
        "\t\tout = self.fc(out)\n",
        "\t\treturn out\n",
        "\n",
        "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# For updating learning rate\n",
        "def update_lr(optimizer, lr):\t\n",
        "\tfor param_group in optimizer.param_groups:\n",
        "\t\tparam_group['lr'] = lr\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate\n",
        "for epoch in range(num_epochs):\n",
        "\tfor i, (images, labels) in enumerate(train_loader):\n",
        "\t\timages = images.to(device)\n",
        "\t\tlabels = labels.to(device)\n",
        "\n",
        "\t\t# Forward pass\n",
        "\t\toutputs = model(images)\n",
        "\t\tloss = criterion(outputs, labels)\n",
        "\n",
        "\t\t# Backward and optimize\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\tweightsSetPositiveModel(model)\n",
        "\n",
        "\t\tif (i+1) % 100 == 0:\n",
        "\t\t\tprint (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
        "\t\t\t\t   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "\t# Decay learning rate\n",
        "\tif (epoch+1) % 20 == 0:\n",
        "\t\tcurr_lr /= 3\n",
        "\t\tupdate_lr(optimizer, curr_lr)\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "\tcorrect = 0\n",
        "\ttotal = 0\n",
        "\tfor images, labels in test_loader:\n",
        "\t\timages = images.to(device)\n",
        "\t\tlabels = labels.to(device)\n",
        "\t\toutputs = model(images)\n",
        "\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\ttotal += labels.size(0)\n",
        "\t\tcorrect += (predicted == labels).sum().item()\n",
        "\n",
        "\tprint('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'resnet.ckpt')"
      ]
    }
  ]
}